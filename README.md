
---

# Predictive Maintenance Project Documentation

### Table of Contents
1. [Introduction](#introduction)
2. [Project Structure](#project-structure)
3. [Installation and Setup](#installation-and-setup)
4. [Data Pipeline](#data-pipeline)
5. [Model Training and Evaluation](#model-training-and-evaluation)
6. [Hyperparameter Tuning for Optimization](#hyperparameter-tuning-for-optimization)
7. [Experiment Tracking and Artifacts](#experiment-tracking-and-artifacts)
8. [Metrics and Model Performance](#metrics-and-model-performance)
9. [Running the Project](#running-the-project)

---

## Introduction

The Predictive Maintenance project aims to forecast machine failures using machine learning, enhancing maintenance schedules, minimizing downtimes, and optimizing operational efficiency. This project follows a well-structured journey, starting with data preparation, progressing through model training, and ultimately reaching the fine-tuning of hyperparameters to achieve the best performance.

---

## Project Structure

The project is organized into folders and scripts that create a seamless flow from data ingestion to evaluation. Each component serves a critical function, making the pipeline easy to understand and modify.

```
project-root/
│
├── .dvc/                           # DVC configuration and cache files
├── .vscode/                        # VSCode workspace settings
├── data/                           # Directory for raw and processed data files
├── src/                            # Source code files
│   ├── autolog.py                  # Script for automated logging with MLflow
│   ├── data_collection.py          # Code for collecting or loading the dataset
│   ├── data_model.py               # Model-related operations like training and evaluation
│   ├── data_pre.py                 # Preprocessing steps for data preparation
│   ├── hyperpara.py                # Hyperparameter tuning code
│   ├── main.py                     # Main script to run the project
│   ├── model_building.py           # Model building and architecture setup
│   ├── model_eval.py               # Model evaluation and performance metrics
│   ├── model_reg.py                # Model registration logic
│   ├── movel_evalautologing.py     # Model evaluation with autologging functionality
│   └── test.py                     # Testing and validation script
│
├── .dvcignore                      # Files and folders to ignore in DVC tracking
├── .gitignore                      # Files and folders to ignore in Git
├── con_met.png                     # Confusion matrix plot as an image artifact
├── dvc.lock                        # DVC lock file
├── dvc.yaml                        # DVC pipeline stages definition
├── metrics.json                    # JSON file storing model evaluation metrics
├── params.yaml                     # Hyperparameters and configuration file
├── requirements.txt                # Python dependencies
├── mlruns/                         # Local directory for MLflow runs and artifacts
├── dvclive/                        # Directory for live DVC metrics
├── plots/metrics                   # Plots generated for metrics tracking
├── mlartifacts/                    # Model artifacts generated by MLflow
│   ├── 207300399835378276/         # Model artifacts for specific runs
│   ├── <hash_id>/artifacts         # Artifacts (e.g., models) for other runs
└── <hash_id>/                      # Unique identifiers for each experiment run
```

---

## Installation and Setup

### Prerequisites
- Python 3.8 or higher
- Git, DVC, and MLflow installed
- [DagsHub](https://dagshub.com/) account for remote experiment tracking

### Step-by-Step Setup

1. **Clone the Repository**  
   ```bash
   git clone <your-repo-url>
   cd predictive_maintenance
   ```

2. **Install Dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up DVC and Data Storage**  
   Configure DVC to manage datasets:
   ```bash
   dvc init
   dvc remote add -d <remote-name> <remote-url>
   dvc pull  # Pull data files if they are already tracked in DVC
   ```

4. **MLflow Setup**  
   Configure MLflow with your DagsHub URI for experiment tracking:
   ```python
   mlflow.set_tracking_uri("https://dagshub.com/username/repo_name.mlflow")
   ```

---

## Data Pipeline

The journey begins with building a robust data pipeline, which ensures high-quality data flows through the model:

1. **Data Collection**  
   The `data_collection.py` script is responsible for sourcing and organizing raw data files into the `data/processed/` folder.

2. **Data Preprocessing**  
   With `data_pre.py`, the data undergoes preprocessing steps, including handling missing values, scaling, and encoding features as required. This processed dataset is now clean and ready for model input.

3. **Data Versioning**  
   DVC tracks changes in the dataset, allowing for easy experimentation with different data versions and ensuring reproducibility across runs.

---

## Model Training and Evaluation

Once the data is ready, we proceed to create and train a baseline model:

1. **Model Initialization**  
   The model setup begins in `model_building.py`, where a `RandomForestClassifier` is defined, with hyperparameters outlined in `params.yaml`.

2. **Training**  
   Training occurs in `main.py`, which orchestrates the entire pipeline, from data loading to model evaluation. Here, we fit the model on the training data and assess it on validation data to ensure it generalizes well.

3. **Initial Model Evaluation**  
   The `model_eval.py` script evaluates the model’s performance using metrics like accuracy, precision, recall, and F1 score. The `metrics.json` file, generated at this stage, holds these results, making it easy to compare baseline and fine-tuned models.

---

## Hyperparameter Tuning for Optimization

Having established a baseline, we move to optimize model performance through hyperparameter tuning:

1. **Defining the Parameter Grid**  
   In `hyperpara.py`, we define a parameter grid to explore potential configurations for the `RandomForestClassifier`, including settings for `n_estimators` and `max_depth`.

2. **Randomized Search**  
   A `RandomizedSearchCV` object tests different configurations. Each run within this search is logged as a nested run using MLflow, allowing us to track each combination's performance metrics and parameters.

3. **Selecting the Best Parameters**  
   The best combination of hyperparameters, recorded in the MLflow experiment logs, is saved as the optimal model for predictive maintenance tasks.

---

## Experiment Tracking and Artifacts

### MLflow Tracking

With each step logged in MLflow, this setup ensures a complete record of model evolution. The `mlruns/` directory houses details on each run, from parameter settings to model performance.

### Artifacts

MLflow and DVC store critical artifacts like the trained model, evaluation plots, and the `con_met.png` confusion matrix. This image file helps visualize the classification success and errors for further insights.

---

## Metrics and Model Performance

The model's performance metrics are recorded in the `metrics.json` file. Here’s an example:

```json
{
    "accuracy": 0.9845,
    "f1_score": 0.7438,
    "precission": 0.7965,
    "recall": 0.6977
}
```

### Metric Descriptions
- **Accuracy**: Measures the ratio of correct predictions to total predictions, indicating general reliability.
- **F1 Score**: The harmonic mean of precision and recall, representing the model’s balance between sensitivity and specificity.
- **Precision**: Indicates the percentage of relevant results in the positive predictions, showing model confidence in predicting machine failure.
- **Recall**: Shows the percentage of actual positives that were correctly classified, reflecting the model’s sensitivity.

---

## Running the Project

1. **Prepare the Data**  
   Make sure the data is properly processed and located in the `data/processed/` directory.

2. **Hyperparameter Tuning**  
   Run `hyperpara.py` to initiate the hyperparameter search.
   ```bash
   python src/hyperpara.py
   ```

3. **Model Training and Evaluation**  
   Run the main pipeline to train the model and evaluate results.
   ```bash
   python src/main.py
   ```

4. **Generate Plots and Artifacts**  
   Artifacts, such as the confusion matrix plot, are generated automatically and saved for review.

---

